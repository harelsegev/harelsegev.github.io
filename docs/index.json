[{"content":"In the README page of my tool, INDXRipper (Go check it out! It\u0026rsquo;s really cool), down in the \u0026ldquo;Limitations\u0026rdquo; section, I gave some warnings regarding the reliability of the tool:\nThe tool may give false results. While false positives are rare, they are possible.\nPartially overwritten entries may not be found. If they are found, though, the tool may give you false information.\nThese shortcomings apply to every $I30 parser, and in this post - I\u0026rsquo;ll explain why. First, I will give an overview of directories in NTFS, and talk a bit about how $I30 parsers work. Then, I\u0026rsquo;ll show you a case where a $I30 parser outputs a false entry, and explain how I try to avoid them in INDXRipper.\nNTFS Directories In NTFS, a directory is ultimately a collection of entries - one (or more) for every file it contains. Those entries live in a sorted data structure called a B-tree, to allow fast access to files. B-trees are similar to binary search trees, except they are not binary - a node can have more than 2 child-nodes:\nAs you can see, each node can store multiple entries. The nodes are laid out sequentially inside an index attribute ($INDEX_ROOT or $INDEX_ALLOCATION), so a $I30 parser can iterate over them without having to deal with the tree structure.\nInside each node, the entries are sorted by filename. As the following diagram illustrates, they can have different sizes - so each entry has a 2-byte field containing the relative offset of the next entry:\nWhen a file is moved in or out of a directory, an entry is created or removed from the tree, respectively. This sometimes causes the tree to rebalance itself - which may cause other, non-related entries to be removed from their node and re-created in another node.\nLet\u0026rsquo;s see what happens if entry B is removed from the above 4-entry node:\nEntries C and D were simply shifted left. Now, this node contains slack space - space at the end of the node that isn\u0026rsquo;t used by either one of the entries, and is marked here with red diagonal stripes. As you can see, it contains a complete copy of entry D and a partially overwritten copy of entry C from before they were shifted.\nThese diagrams showcase some of the unique qualities of this artifact:\nNot every removed entry will have remnants in slack space (Entry B) Entries that were not removed may have remnants in slack space (Entry C and D) Entries in slack space may be partially overwritten (Entry C) Carving Entries From $I30 Slack Space As we have just seen, the content of $I30 slack space is unpredictable. To recover file metadata from it, $I30 parsers utilize techniques that are very similar to file carving.\nFile carvers often rely on file signatures to detect files in unallocated space. Unfortunately, $I30 entries don\u0026rsquo;t have a standard header or a fixed signature they can be detected with. However, the file metadata they contain can itself be used to build carving rules.\nMost $I30 parsers base their carving rules on the set of MACB timestamps that $I30 entries contain. A sequence of 4 timestamps makes an really good carving rule, because timestamps only make sense within a very small range of values.\nA Windows FILETIME timestamp has a range of +/- 29,227 years from its epoch of 1601. If you find a sequence of 4 values that are all within a reasonable range when interpreted as timestamps (say, 1990 - 2030), that\u0026rsquo;s probably not a coincidence.\nFalse Entries Sometimes it is a coincidence, though. Here\u0026rsquo;s a snippet from a really weird output I got from INDXParse when I executed it on the index attribute of a directory on my system:\nThe file in the first line looks normal. The last two lines, however - are messed up:\nThe second line is almost completely empty, except an unprintable character and a lowercase O in the filename field. The file in the third line has a physical size of roughly 118 Petabytes, which is a tiny bit more than my hard drive\u0026rsquo;s storage capacity. Its name looks cut-off, and a creation time of 2015 is odd, considering all the other files in that directory were created much later. The entry this data was pulled from is partially overwritten - which might explain this output. Let\u0026rsquo;s take a look at this entry in hex, and try to figure out what\u0026rsquo;s going on:\nWhat\u0026rsquo;s marked in yellow is a special entry indicating the end of a node. Everything below it is in slack space - including the entry we\u0026rsquo;re interested in. Although it\u0026rsquo;s partially overwritten, the file metadata inside our entry is still intact:\nThe filename is marked in blue. The 4 MACB timestamps are marked in green. The file\u0026rsquo;s physical size is marked in brown. The red byte is the filename length in characters. What\u0026rsquo;s marked in purple is the file reference of the parent directory. It\u0026rsquo;s composed of the directory\u0026rsquo;s MFT entry number (first 6 bytes) and a sequence number (last 2 bytes).\nAll of this file metadata is completely valid. This is the output we would have got if this entry were parsed correctly:\nFilename Physical Size Logical Size Creation Time AlarmsAppList.targetsize-16_contrast-black.png 4096 243 06/03/2020 INDXParse should have detected the set of 4 MACB timestamps, and should have given us this output. What went wrong? Well\u0026hellip; everything.\nBy pure chance, the file reference of the parent directory is also a reasonable timestamp:\nLooks familiar? It\u0026rsquo;s the weird creation time from the third output line! INDXParse detected this file reference as a timestamp. The fields that come after it are the actual timestamps, so that lead to the detection of a wrong set of 4 timestamps:\nThis - my friends, is a false entry. It overlaps with our real entry - which makes this a bit confusing. The fields marked here belong to the false entry. Looking at them, it becomes clear that INDXParse detected this false entry instead of the real one.\nLet\u0026rsquo;s take a second look at the output of INDXParse:\nAs we have just seen, the creation time in the false entry - 54 2B 00 00 00 00 D1 01, matches the creation time in the third output line, and is actually the parent directory reference in the real entry. The physical size in the false entry (marked in brown) matches the physical size in the third output line (~118 petabytes). This value is the last access time in the real entry, which explains why it is so ridiculous as a file size. The filename (marked in blue) starts with 0x04, which is an EOT character. Then there\u0026rsquo;s 0x6F, which is a lowercase O, and 0x0D - a carriage return character (CR), which excel interprets as a new line. This character sequence is actually the second output line. The rest of the filename is in the third output line. It\u0026rsquo;s cut-off, because the filename length in the false entry (marked in red) is a lot shorter than the real one. Avoiding False Entries Unfortunately, false entries cannot be entirely avoided. When carving data from an unstructured binary blob, there\u0026rsquo;s always a risk of false positives. Nevertheless, the false positive rate can be reduced with more in-depth validation of the carved data.\nThe false entry shown above can be easily identified as one:\nThe filename contains control characters. While most of the characters in the Unicode \u0026ldquo;Control\u0026rdquo; category are allowed in filenames, they are very rarely used in them. The file\u0026rsquo;s physical size, besides being absolutely ridiculous, is not a multiple of the file-system\u0026rsquo;s cluster size. This is an obvious red flag. I implemented similar validations in my tool, INDXRipper - so in this case, it ignores the false entry and outputs the real one, as it should. Those validations aren\u0026rsquo;t foolproof, though. $I30 slack space is unpredictable, so there\u0026rsquo;s always a chance they will fail.\n","permalink":"https://harelsegev.github.io/posts/i30-parsers-output-false-entries.-heres-why/","summary":"In the README page of my tool, INDXRipper (Go check it out! It\u0026rsquo;s really cool), down in the \u0026ldquo;Limitations\u0026rdquo; section, I gave some warnings regarding the reliability of the tool:\nThe tool may give false results. While false positives are rare, they are possible.\nPartially overwritten entries may not be found. If they are found, though, the tool may give you false information.\nThese shortcomings apply to every $I30 parser, and in this post - I\u0026rsquo;ll explain why.","title":"$I30 Parsers Output False Entries. Here's Why"},{"content":"WSH (Windows Script Host) is an automation tool built into Windows, providing powerful scripting abilities. It was introduced in Windows 98, long predating .NET and PowerShell. Whilst being largely abandoned by system administrators, It is sometimes used by attackers to evade detection and obfuscate their infection chains.\nTypically, an attacker will drop a malicious script on disk - a .vbs, .js or a .wsf file, and then execute it using either the WScript or the CScript host.\nSystem-wide WSH settings are stored in the SOFTWARE hive, in the Windows Script Host key, and can be examined using the wsh_settings RegRipper plugin:\nThis post is not about this key, though. It\u0026rsquo;s about a matching Windows Script Host key in the NTUSER.DAT hive, which stores the user-specific WSH settings:\nInterestingly, the WSH key in NTUSER.DAT does not exist by default. It is only created when the user executes WScript or CScript for the first time.\nBecause they have their own sub-key, a change in the settings does not affect the last write timestamp of the WSH key. It won\u0026rsquo;t usually change after the key is created, even if the settings are changed.\nThe big question is - can we use this timestamp to determine the first time a user executed WScript or CScript? Well, maybe. It depends on whether values or sub-keys other than Settings are ever created beneath the WSH key.\nI\u0026rsquo;ve done some testing, and I couldn\u0026rsquo;t get either WScript or CScript to create (or even query) other values or sub-keys. I believe it never happens. I cannot be entirely sure, but I\u0026rsquo;m somewhat confident.\nIf I\u0026rsquo;m right - then the last write time of the WSH key always indicates its creation time, and I can see all sorts of exciting things we can use this for:\nDetermine the first time a user executed WScript or CScript. If WSH isn\u0026rsquo;t typically used in your environment, this may be worth hunting for. Attribute the execution of a WSH script to a specific user account. If a .js script were dropped on disk, and a user\u0026rsquo;s WSH key were then immediately created, I\u0026rsquo;d bet that user executed the script. Note: If WScript or CScript is executed with SYSTEM privileges, the Windows Script Host key will be created in the DEFAULT hive.\n","permalink":"https://harelsegev.github.io/posts/the-forensic-value-of-the-other-wsh-registry-key/","summary":"WSH (Windows Script Host) is an automation tool built into Windows, providing powerful scripting abilities. It was introduced in Windows 98, long predating .NET and PowerShell. Whilst being largely abandoned by system administrators, It is sometimes used by attackers to evade detection and obfuscate their infection chains.\nTypically, an attacker will drop a malicious script on disk - a .vbs, .js or a .wsf file, and then execute it using either the WScript or the CScript host.","title":"The Forensic Value of the (Other) WSH Registry Key"},{"content":"I was working on a case the other day, when I first came across a rather interesting registry key, HKLM\\Software\\Microsoft\\RADAR\\HeapLeakDetection\\DiagnosedApplications. It caught my eye, because it has sub-keys for (what appears to be) applications executed on the system. This is what it looks like on my own system:\nIt has quite a few sub-keys, and each one has a LastDetectionTime QWORD value, containing what appears to be a Windows FILETIME timestamp:\nThis key is associated with the Memory Leak Diagnoser component of Windows Resource Exhaustion Detection and Resolution (RADAR). RADAR is a technology embedded in Windows to detect memory leaks in real-time, so that data can be collected and used to correct issues in applications.\nRADAR is surprisingly old; it was introduced in Windows Vista. Nevertheless, I couldn\u0026rsquo;t find any research into this registry key - so I had to conduct my own. With forensics in mind, I tried to answer these 2 questions:\nUnder what conditions is a sub-key created beneath the DiagnosedApplications key? Under what conditions is the LastDetectionTime value updated? My research was conducted on Windows 10 machines, and therefore may not apply to prior versions of Windows.\nLooking at the Settings Luckily, there\u0026rsquo;s a Settings key beneath the HeapLeakDetection key:\nSettings are always helpful when you\u0026rsquo;re trying to figure out how something works. Looking at these values, I already had some hypotheses. I figured whether an application is diagnosed has something to do with the amount of memory it allocated.\nI had a guess that CommitFloor and CommitCeiling are actually in megabytes, and that the amount of committed memory has to lie between them for a process to be registered. That lead me to write a little C program, to test this theory:\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;Windows.h\u0026gt; int main() { // set nAllocatedSize to be between CommitFloor and CommitCeiling size_t nAllocatedSize = 1024 * 1024 * 250; if(VirtualAlloc(NULL, nAllocatedSize, MEM_RESERVE | MEM_COMMIT, PAGE_READWRITE)) { printf(\u0026#34;Committed 250 MB of memory\\n\u0026#34;); while(1) { printf(\u0026#34;Sleeping...\\n\u0026#34;); Sleep(5000); } } } I executed the binary on 2 of my machines, and the results threw me off completely. On one of them, it was registered after a few minutes - but on the other one it wasn\u0026rsquo;t registered at all. They\u0026rsquo;re both running the same Windows 10 version, so what\u0026rsquo;s going on?\nReverse Engineering RdrpReadHeapLeakSettings To figure this out, I needed more information. Using Procmon, I was able to pinpoint the DLL which manages the DiagnosedApplications registry key:\nI figured I should start my analysis in the RdrpReadHeapLeakSettings function. I hoped it would help me understand the settings better. Take DetectionInterval, for example; it\u0026rsquo;s 0x1E, or 30 decimal - but 30 what? seconds? minutes? I had no idea.\nThis turned out to be a great idea, because the function transforms each value in a way that enabled me to understand what it means. For example, the DetectionInterval value is multiplied by 86400 after it is read from the registry. 86400 happens to be 60 * 60 * 24. This means DetectionInterval is stored days, and is converted to seconds:\nIn a similar fashion, I was able to conclude that TimerInterval is stored in minutes.\nDetectionInterval is stored in a global variable after it\u0026rsquo;s converted to seconds - to make it available for other functions to use. This is also the case for CommitThreshold, MaxReports and TimerInterval. However, both CommitFloor and CommitCeiling aren\u0026rsquo;t stored anywhere after they\u0026rsquo;re read. I doubt whether they\u0026rsquo;re actually used at all!\nThe next value I looked at is CommitThreshold, which seems to be 5 by default. After it is read from the registry, the function checks if it\u0026rsquo;s between 1 to 100:\nAfter that, there\u0026rsquo;s a call to NtQuerySystemInformation to get a SystemBasicInformation struct. Unfortunately, the structure of this struct is not documented on MSDN:\nNevertheless, I found documentation for it on some other, rather questionable website. Then, I was able to define it as a struct in IDA Pro. What I found then, was the explanation to the confusing results of my test:\nCommitThreshold is multiplied by (PageSize * NumberOfPhysicalPages) / 100. That lead me to conclude it\u0026rsquo;s the amount of memory an application has to commit in order to get diagnosed, as a percentage of the total amount of physical memory on the system.\nI was able to verify this theory through further testing. This artifact behaves differently depending on the amount of RAM installed in the machine.\nThe 250MBs my test application allocated are more than 5 percent out of the 4GBs of RAM installed in the first machine I tested on, but they\u0026rsquo;re less than 5 percent out of the 8GBs installed in my other machine.\nLooking at RdrpIdentifyTargetProcess What happens if the Memory Leak Diagnoser is triggered, and there are multiple processes with a high enough amount of committed memory to be diagnosed? Well, only one of them is chosen to be registered. I wanted to figure out how this choice is made, so the next function I looked at was RdrpIdentifyTargetProcess.\nFirst, it calls RdrpGetProcessInformation, which in turn uses the NtQuerySystemInformation API call; this time - to get an array of SYSTEM_PROCESS_INFORMATION structures, one for each running process:\nThese structures contain information about the resource usage of each process. Later, it seems like this array is sorted by the amount of committed memory in each process:\nAnswering my Research Questions At this point, I had a solid and complete theory, which I have tested and verified:\nThe Memory Leak Diagnoser is triggered every TimerInverval minutes. When triggered, it gets a list of all running processes, and sorts them by their amount of committed memory (in descending order). Then, it chooses the first process on the list which:\nHas committed at least CommitThreshold percent out of the total amount of physical memory installed. Wasn\u0026rsquo;t already chosen in the last DetectionInterval days. A sub-key beneath the DiagnosedApplications key is then created (if it doesn\u0026rsquo;t exist already) for the chosen process, and the LastDetectionTime is updated.\nEdit: some built-in executables are chosen to be diagnosed, but are not registered.\nWhy? Further research is needed.\nClosing Thoughts That\u0026rsquo;s all! I hope you found this short research interesting. This artifact is not all that powerful, because only the applications that commit a large amount of memory are registered. Nonetheless, it was quite fun to research. I\u0026rsquo;m not super experienced in using IDA and reverse engineering, but I think I did a decent job. Let me know what you think!\n","permalink":"https://harelsegev.github.io/posts/the-mystery-of-the-heapleakdetection-registry-key/","summary":"I was working on a case the other day, when I first came across a rather interesting registry key, HKLM\\Software\\Microsoft\\RADAR\\HeapLeakDetection\\DiagnosedApplications. It caught my eye, because it has sub-keys for (what appears to be) applications executed on the system. This is what it looks like on my own system:\nIt has quite a few sub-keys, and each one has a LastDetectionTime QWORD value, containing what appears to be a Windows FILETIME timestamp:","title":"The Mystery of the HeapLeakDetection Registry Key"},{"content":"In NTFS, the MFT (Master File Table) is a structure that contains a lot of the file-system metadata, and also the contents of small files. It is stored in a special file, called $MFT. In incident response, we often collect and parse this file to determine the file system contents and how it changed over time, without having to acquire a full disk image.\nThere are many bad MFT parsers out there. This is no coincidence - an MFT parser is an extremely easy thing to mess up. Most MFT parsers don\u0026rsquo;t bother handling the uncommon edge cases - and they work fine, most of the times. The fact is, some of those uncommon edge cases are actually more common than you would think. Handling them correctly is what sets the good MFT parsers apart from the bad ones.\nThe MFT is an array of file records - each one describes a file on the system. File records do not to store directory hierarchy, though. There\u0026rsquo;s another structure for that - directory indexes. The directory index a file is listed in determines its location in the directory tree. The directory index is the one storing the name of the file, inside a structure called a $FILE_NAME attribute.\nReconstructing full file paths using only the MFT is not trivial. Whether it is a reliable method at all is debatable. It is only possible because of an NTFS quirk - There\u0026rsquo;s a backup of every file\u0026rsquo;s $FILE_NAME attribute inside its MFT record. Nonetheless, this method is simple, fast, and is often good enough. In this post, I\u0026rsquo;ll help you avoid its common implementation pitfalls.\nBefore we Start Given a $MFT file, we first have to determine the size of an individual file record. It\u0026rsquo;s usually 1024 bytes, but it doesn\u0026rsquo;t have to be. This value is stored in the file system boot sector - which we don\u0026rsquo;t have. Instead, we can find any two consecutive file records, and calculate the difference between their offsets. This should give us the size of a file record.\nAnother thing to consider, is that file records, like other important file system structures, use fixup values to detect disk errors. If you ignore fixup values, your parser will output incorrect data, and the worst part is - you are probably not going to notice. Take my word for it, and start writing the code to deal with fixup values early on.\nA Naive Approach This is the structure of a $FILE_NAME attribute:\nOffset Size Description 0x00 6 Parent index 0x06 2 Parent sequence 0x08 8 Creation time 0x10 8 Modification time 0x18 8 MFT record change time 0x20 8 Access time 0x28 8 Allocated size 0x30 8 Real size 0x38 4 Flags 0x3C 4 Used by EAs and Reparse 0x40 1 Filename length in bytes / 2 (L) 0x41 1 Filename namespace 0x42 2L Filename in Unicode (not null terminated) As mentioned earlier, each file has a $FILE_NAME attribute inside its MFT record. For now, we\u0026rsquo;re only interested in these 2 fields:\nFilename\nThe name of the file described by the record.\nParent index\nAn index to the MFT record of the file\u0026rsquo;s parent folder.\nUsing these fields, we can build the full path of a file:\n+--------------+ +----------\u0026gt;| . | 4. .\\Documents\\PDF\\document.pdf | +--------------+ | +-------\u0026gt;| Documents | 3. Documents\\PDF\\document.pdf +--|--------+--------------+ | | | | +--------------+ | +----\u0026gt;| PDF | 2. PDF\\document.pdf +--|-----+--------------+ | | | | +--------------+ | | | | +--------------+ | | document.pdf | 1. document.pdf +-----+--------------+ In NTFS, \u0026#34;.\u0026#34; is the file name of the root directory. By looping over the MFT records, we can do this for every file on the system. Here\u0026rsquo;s a Python-style pseudo code of the algorithm:\nfor current_record in mft.get_records(): print(build_path(mft, current_record)) def build_path(mft, current_record): path = \u0026#34;\u0026#34; while not is_root_directory(current_record): filename_attribute = current_record.get_attribute(\u0026#34;$FILE_NAME\u0026#34;) path = \u0026#34;\\\\\u0026#34; + filename_attribute.get_field(\u0026#34;Filename\u0026#34;) + path current_record = mft.get_record_at(filename_attribute.get_field(\u0026#34;Parent index\u0026#34;)) return \u0026#34;.\u0026#34; + path Of course, we could have used caching to improve the efficiency - but for the sake of simplicity, all of the code in this post will be pretty inefficient. In the next sections, we\u0026rsquo;ll see how deleted files, hard links and extension records break everything, and we\u0026rsquo;ll change our code to deal with them.\nPitfall 1: Orphan Files \u0026amp; the Sequence Number The first edge case we\u0026rsquo;ll discuss involves deleted files. When a file is deleted, its MFT record is marked as free. Then, it can be reused if a new file is created - why add a new record to the MFT when there\u0026rsquo;s a free, existing one? If there are multiple free records, and a new file is created - it may occupy either one of them.\nWe can tell whether a record is in use by looking at the Flags field in the record header, and more specifically - at its least significant bit, which is the flag representing the state of the record. When a record is allocated to a file, it is set. When that file is deleted, it is turned off.\nOrphan Files As long as the record of a deleted file is not reused - it remains intact, which means we can resolve the file\u0026rsquo;s path just like any other file. Problems may arise if the file\u0026rsquo;s parent folder is deleted as well. Let\u0026rsquo;s see what happens when the parent folder\u0026rsquo;s record is reused, while the child\u0026rsquo;s record remains free:\nFolder1 is created. It occupies the MFT record at index 41\nRecord state Record index Reconstructed file path In use 41 .\\Folder1 File1.txt is created inside Folder1. It occupies the MFT record at index 42\nRecord state Record index Reconstructed file path In use 41 .\\Folder1 In use 42 .\\Folder1\\File1.txt Folder1 is deleted. File1.txt is in Folder1, so it is also deleted\nRecord state Record index Reconstructed file path Free 41 .\\Folder1 Free 42 .\\Folder1\\File1.txt Folder2 is created. It occupies the MFT record at index 41\nRecord state Record index Reconstructed file path In use 41 .\\Folder2.txt Free 42 .\\Folder2\\File1.txt File1.txt was never in Folder2!\nDeleted files their parent folder\u0026rsquo;s record was reused are called Orphan Files. They were once created in some folder, but it is now gone. their original path cannot be resolved. A folder can be orphan too, which will make it the root of an orphan sub-tree. The hierarchy of the files inside an orphan sub-tree can be reconstructed, but it\u0026rsquo;s detached from the main directory tree. the original path of the root of the sub-tree, which is an orphan folder, cannot be resolved.\nThe Sequence Number On its own, an MFT index is not enough to reference a file. It references a record, which can be reused to describe many files over time. We don\u0026rsquo;t want to reference records, we want to reference the files they describe. To reference a file, not only do we need to know its MFT index, but also whether the record in that index still belongs to it, or has been reused since.\nThis is where the sequence number comes in. It is stored in the record header, and is used together with the MFT index to reference files. That\u0026rsquo;s why together, they are called a File Reference. Every time a record is freed, its sequence number is increased by 1. Then, any file references with the old sequence number are considered invalid.\nHere\u0026rsquo;s a pseudo-code of a reference validation function:\ndef is_valid_file_reference(mft, mft_index, sequence_number): file_record = mft.get_record_at(mft_index) if file_record.is_used(): return file_record.get_sequence_number() == sequence_number else: # the sequence number was increased when the record was freed, # so we should decrease it to get the actual sequence number of the file return file_record.get_sequence_number() - 1 == sequence_number Look again at the structure of the $FILE_NAME attribute. Not only do we have the MFT index of the parent folder, but also its sequence number. In other words - we have a file reference to the parent folder. This is exactly what we need to identify orphan files - all we need to do is validate this reference. If it\u0026rsquo;s not valid, it means the parent folder was deleted, and its record was reused.\nNow that we can identify orphan files, we have to decide how our code should deal with them. We could output the filenames and paths separately, and just not output a path for them. Or maybe we can place them all in some special folder, but which one? There\u0026rsquo;s no folder it would be right to place them in. The solution we\u0026rsquo;ll go with is quite simple: we\u0026rsquo;ll just create a new folder for them.\nOf course, it won\u0026rsquo;t be an actual file system folder - we cannot create these. Nevertheless, we are the ones building the directory tree, so we can just \u0026ldquo;make up\u0026rdquo; a folder. It would be a \u0026ldquo;virtual folder\u0026rdquo;, if you will. We\u0026rsquo;ll call it \u0026ldquo;$OrphanFiles\u0026rdquo; and place it in the root directory. Now we have a foster parent for all the orphan files!\nHere\u0026rsquo;s our upgraded code:\nfor current_record in mft.get_records(): print(build_path(mft, current_record)) def build_path(mft, current_record): path = \u0026#34;\u0026#34; while not is_root_directory(current_record): filename_attribute = current_record.get_attribute(\u0026#34;$FILE_NAME\u0026#34;) path = \u0026#34;\\\\\u0026#34; + filename_attribute.get_field(\u0026#34;Filename\u0026#34;) + path current_record = mft.get_record_at(filename_attribute.get_field(\u0026#34;Parent index\u0026#34;)) sequence_number = current_record.get_sequence_number() if not current_record.is_used(): sequence number -= 1 if sequence_number != filename_attribute.get_field(\u0026#34;Parent sequence\u0026#34;): path = \u0026#34;\\\\$OrphanFiles\u0026#34; + path break return \u0026#34;.\u0026#34; + path Pitfall 2: Hard Links and Short Names Hard Links Hard links are an often overlooked feature of NTFS. They are similar in functionality to the hard links in other file systems - a file can be located in multiple folders at the same time, and can have a different name in each one. Each directory index a file is located in stores a $FILE_NAME attribute for it. Fortunately for us, we have copies of all of these $FILE_NAME attributes in the file\u0026rsquo;s MFT record.\nThis means a file can have multiple paths. To be exact, a file can have multiple hard links, and each one of them has a path. Each hard link has its own $FILE_NAME attribute, containing its own file name and parent folder. To build all of the paths for a file, we\u0026rsquo;ll loop over every $FILE_NAME attribute in its MFT record, and use them to build the path of each hard link separately.\nFolders are easier to handle, because NTFS does not allow multiple hard links for them - it would allow you to create loops in the directory tree, and nobody wants that. This means a folder has only a single path. A hard link has a single parent folder - so it also has a single path. This means we\u0026rsquo;ll build the path of each hard link very similarly to how we did it previously.\nShort Names Short file names (SFNs) are a special type of hard links. If enabled, they\u0026rsquo;re created automatically for files their names are too long to be handled by some old applications. They provide these old applications with an auto-generated, shorter file name they can handle. A short name is created in the same directory with the original hard link, and we can build the path for it the same way we do for a regular hard link.\nWe can tell whether a $FILE_NAME attribute contains a short name by checking its Filename namespace flag. A DOS namespace means it\u0026rsquo;s a short name, and a WIN32 or a POSIX namespace means it\u0026rsquo;s a regular, long name. WIN32 \u0026amp; DOS is a special value indicating there is no short name for the file, because its name is already a valid short name.\nFolders cannot have regular hard links, but they can have short names. This is a problem - if folders can have 2 different names, it means a hard link can have a huge number of different paths. Suppose a hard link is n folders deep in the directory tree, and each folder has 2 names at most - long and short, excluding the root directory, a hard link has at most 2ⁿ⁻¹ different paths!\n. \\ ??? \\ ??? \\ ??? \\ .... \\ File.txt x2 x2 x2 .... To keep things reasonable, we\u0026rsquo;ll only use the long name of each folder to build the path of a hard link. By doing so, we\u0026rsquo;re in fact choosing a single path for it, out of the many it may have. We can get away with it, because we build the paths for the folders in the exact same way. Each folder\u0026rsquo;s short name is also a hard link, and we\u0026rsquo;ll build a path for it when we get to that folder\u0026rsquo;s MFT record. We\u0026rsquo;re not loosing any short names.\nLong names with the same prefix will have similar short names generated for them. It\u0026rsquo;s important to output the MFT index of each path, so we can tell which short name belongs to which long name:\nMFT Index Path 59:\t.\\Program Files 1293:\t.\\ProgramData 59:\t.\\PROGRA~1 1293:\t.\\PROGRA~2 PROGRA~1 --\u0026gt; Program Files PROGRA~2 --\u0026gt; ProgramData This is the upgraded code. It now handles hard links, and outputs both long and short names:\nfor current_record in mft.get_records(): build_path(mft, current_record) def build_path(mft, current_record): mft_index = current_record.get_mft_index() for filename_attribute in current_record.get_filename_attributes(): print(mft_index + \u0026#34;: \u0026#34; + build_path_helper(mft, filename_attribute)) def build_path_helper(mft, filename_attribute): path = \u0026#34;\u0026#34; while True path = \u0026#34;\\\\\u0026#34; + filename_attribute.get_field(\u0026#34;Filename\u0026#34;) + path parent_record = mft.get_record_at(filename_attribute.get_field(\u0026#34;Parent index\u0026#34;)) if is_root_directory(parent_record): break parent_sequence = parent_record.get_sequence_number() if not parent_record.is_used(): parent_sequence -= 1 if parent_sequence != filename_attribute.get_field(\u0026#34;Parent sequence\u0026#34;): path = \u0026#34;\\\\$OrphanFiles\u0026#34; + path break filename_attribute = next(get_lfn_filename_attributes(parent_record)) return \u0026#34;.\u0026#34; + path # Get only filename attributes with a Long File Name (LFN) def get_lfn_filename_attributes(file_record): for filename_attribute in file_record.get_filename_attributes(): if filename_attribute.get_field(\u0026#34;Name space\u0026#34;) != \u0026#34;DOS\u0026#34;: yield filename_attribute If you don\u0026rsquo;t care about the short names, you don\u0026rsquo;t have to build paths for them. You can simply ignore them altogether. That\u0026rsquo;s perfectly fine, but I believe it\u0026rsquo;s better to output both long and short names - and group them into a single event, later in the analysis phase. To ignore short names, all we need is a small modification to the build_path function:\ndef build_path(mft, current_record): for filename_attribute in get_lfn_filename_attributes(current_record): print build_path_helper(mft, filename_attribute) Pitfall 3: Extension Records, Missing Attributes and Orphaned Attributes Extension Records If a file becomes extremely large and fragmented, or has many hard links, a single file record may not be big enough to contain all of its attributes. In such case, the file will occupy additional file records. The file\u0026rsquo;s main record is called the Base Record, and the additional ones are called Extension Records. When some of a file\u0026rsquo;s attributes are stored in extension records, its base record contains a special attribute, called $ATTRIBUTE_LIST, which stores the information needed to find them.\nSome MFT parsers use the $ATTRIBUTE_LIST attribute in the base record to find its extension records. This is a major mistake, because $ATTRIBUTE_LIST attributes can be non-resident (stored outside the MFT). Instead, we\u0026rsquo;ll take the opposite approach - find the extension records, and trace them back to their base record.\nConveniently, the file-record header contains a Base index and Base sequence fields, to store a file reference to the base record. In base records, these fields are zeroed out. This means we can also use them to differentiate base records from extension records, without having to check for a $ATTRIBUTE_LIST attribute.\nBy looping over the MFT, we can create a mapping of base records to extension records:\n# a dictionay to map base records to their extension records # mappings are of the form: (n, m) -\u0026gt; [k1, k2, ...], where: # (n, m) is a file reference of a base record # ki is the MFT index of an extension record extension_records = {} for current_record in mft.get_records(): if not is_base_record(current_record): base_index = current_record.get_base_record_index() base_sequence = current_record.get_base_record_sequence() (extension_records .setdefault((base_index, base_sequence), []) .append(current_record.get_mft_index())) def is_base_record(current_record): return (current_record.get_base_record_index() == 0 and current_record.get_base_record_sequeunce() == 0) After we create this dictionary, we have to loop over the MFT one more time. This time, looking for base records. For each base record, we will build a path for all the $FILE_NAME attributes both in it, and in its extension records - if it has any.\nMissing Attributes As always, deleted files create problems. When a file is deleted, its base record and all of its extension records are marked as free. The problem is - all of those free records are not going to be reused all at the same time. If some, but not all of the file\u0026rsquo;s records are reused - we loose some, but not all of the file\u0026rsquo;s attributes. This has major consequences on MFT parsing in general, and on path reconstruction in particular.\nWe may find a file without any $FILE_NAME attributes. For such a file, path reconstruction is impossible. The best you can do is give it a unique identifier. You may be tempted to place it in the $OrphanFiles folder, but that wouldn\u0026rsquo;t be right - it\u0026rsquo;s not necessarily orphan. Sure, it\u0026rsquo;s deleted, but its parent folder might not be.\nIf you find a folder without any $FILE_NAME attributes, you may also find deleted files that were once inside it. Those files can be safely placed in the $OrphanFiles folder, because the folder is deleted, and we won\u0026rsquo;t output a path for it.\nOrphaned Attributes To resolve the parent folder for a file, we validate the parent reference in its $FILE_NAME attribute. If it\u0026rsquo;s not valid, it means the file is orphan, right? Well, yes - but we can do better than that. Before declaring the file is orphan, we should check whether its parent has any extension records left. If there\u0026rsquo;s a $FILE_NAME attribute in one of them, we can use it to resolve the file\u0026rsquo;s path.\nThis is the final version of our code. It handles everything we talked about so far: orphan files, hard links and extension records:\n# a dictionay to map base records to their extension records # mappings are of the form: (n, m) -\u0026gt; [k1, k2, ...], where: # (n, m) is a file reference of a base record # ki is the MFT index of an extension record extension_records = {} # first pass over the MFT for current_record in mft.get_records(): if not is_base_record(current_record): base_index = current_record.get_base_record_index() base_sequence = current_record.get_base_record_sequence() (extension_records .setdefault((base_index, base_sequence), []) .append(current_record.get_mft_index())) # second pass over the MFT for current_record in mft.get_records(): if is_base_record(current_record): build_path(mft, current_record) def is_base_record(current_record): return (current_record.get_base_record_index() == 0 and current_record.get_base_record_sequeunce() == 0) def build_path(mft, current_record): base_filename_attributes = current_record.get_filename_attributes() extension_filename_attributes = get_extension_filename_attributes( current_record.get_mft_index(), current_record.get_sequence_number() ) mft_index = current_record.get_mft_index() for filename_attribute in base_filename_attributes + extension_filename_attributes: print(mft_index + \u0026#34;: \u0026#34; + build_path_helper(mft, filename_attribute)) def build_path_helper(mft, filename_attribute): path = \u0026#34;\u0026#34; while True path = \u0026#34;\\\\\u0026#34; + filename_attribute.get_field(\u0026#34;Filename\u0026#34;) + path parent_record = mft.get_record_at(filename_attribute.get_field(\u0026#34;Parent index\u0026#34;)) if is_root_directory(parent_record): break parent_sequence = parent_record.get_sequence_number() if not parent_record.is_used(): parent_sequence -= 1 parent_filename_attributes = [] # if parent record still belongs to my parent, get its filename attributes if parent_sequence == filename_attribute.get_field(\u0026#34;Parent sequence\u0026#34;): parent_filename_attributes = parent_record.get_filename_attributes() # get the filename attributes from any extension records of my parent parent_filename_attributes += get_extension_filename_attributes( filename_attribute.get_field(\u0026#34;Parent index\u0026#34;), filename_attribute.get_field(\u0026#34;Parent sequence\u0026#34;) ) if not parent_filename_attributes: # could not find a $FILE_NAME attribute of my parent path = \u0026#34;\\\\$OrphanFiles\u0026#34; + path break # prioritize LFNs when choosing a name for the parent filename_attribute = max(parent_filename_attributes, key=get_priority) return \u0026#34;.\u0026#34; + path def get_extension_filename_attributes(base_index, base_sequence): extension_records = map( mft.get_record_at, extension_records[(base_index, base_sequence)] ) filename_attributes = [] for record in extension_records: filename_attributes += record.get_filename_attributes() return filename_attributes def get_priority(filename_attribute): if filename_attribute.get_field(\u0026#34;Name space\u0026#34;) == \u0026#34;DOS\u0026#34;: return 0 else: return 1 In the previous version, we built paths using the long file name (LFN) of each folder, because the long names are more meaningful than the auto-generated short names. Now we know that the $FILE_NAME attribute containing the short name may be the only $FILE_NAME attribute left after some of a folder\u0026rsquo;s records were reused.\nIn this new version, we prioritize the LFN $FILE_NAME attributes, but we will use the SFN $FILE_NAME attribute if it\u0026rsquo;s the only one left.\nClosing Thoughts This post was my attempt to create the resource I wish I had when I wrote my MFT parser. MFT parsing is such a fundamental technique in forensics; yet, the resources to learn it are lacking. People have been writing MFT parsers for years, each one on their own, rediscovering the same edge cases and learning the same lessons again and again.\nIt really shouldn\u0026rsquo;t be like that. I think we DFIR programmers have a lot to learn in sharing our knowledge with one another, so we can push the field forward, together.\n","permalink":"https://harelsegev.github.io/posts/resolving-file-paths-using-the-mft/","summary":"In NTFS, the MFT (Master File Table) is a structure that contains a lot of the file-system metadata, and also the contents of small files. It is stored in a special file, called $MFT. In incident response, we often collect and parse this file to determine the file system contents and how it changed over time, without having to acquire a full disk image.\nThere are many bad MFT parsers out there.","title":"Resolving File Paths Using the MFT"},{"content":"I often test my tools on my old computer at home. It\u0026rsquo;s so much more interesting to investigate than a newly created virtual machine. Today, while testing, I found evidence of activity from almost 2 years ago. It got me really excited, and I thought it would make a cool blog post!\nHere\u0026rsquo;s a snippet from the timeline I created using MFTECmd and INDXRipper:\nA Prefetch File in $I30 Slack The Prefetch file FLOSS64.EXE-F5F88991.pf was created on September 10th 2020. Today is July 2nd 2022; almost 2 years have passed! There\u0026rsquo;s no trace of this file in the MFT, nor in $J - which goes only a week back. The only thing left proving this file once existed is a $I30 entry in slack space of the Prefetch folder.\nThis Prefetch file must have been created because I executed Floss, which is a tool from FireEye. But floss64.exe is not in the MFT or in $J. It\u0026rsquo;s not in my timeline, meaning INDXRipper could not find it in $I30 of any directory. I parsed the ShimCache and AmCache, but there was nothing there either.\nThe $I30 of the Prefetch folder is a forensic goldmine; The $I30s from the users\u0026rsquo; Recent folders are also very valuable. When triaging a system, I usually collect the $I30s from such key folders, these are just a few examples:\nC:\\Windows\\Prefetch C:\\Users\\*\\AppData\\Roaming\\Microsoft\\Windows\\Recent C:\\ProgramData\\Microsoft\\Windows Defender\\LocalCopy C:\\Users\\*\\Downloads C:\\PerfLogs I use RawCopy to collect these; KAPE does not currently support $I30 collection.\nThe Prefetch file was created at 23:14:57, probably right after floss64.exe was first executed. We can also see a last modification time at 23:25:35. If I were to find this Prefetch file in the MFT, and this last modification timestamp were from $STANDATD_INFORMATION, I would say that it indicates the last execution time of floss64.exe. However, this timestamp is from $I30 slack, so this might not be the case.\nI know what you think - $I30 timestamps should mirror those in the $STANDATD_INFORMATION attribute! And you are absolutely right. But $I30 entries, as opposed to MFT entries, can move around throughout their lifetime. When a file is created or deleted in a directory, $I30 entries of other files in the directory may be unallocated, and then reallocated in a different place. This entry could have been unallocated while the Prefetch file was still active! In this case, the entry may contain outdated information.\nPyInstaller Floss is written in Python, and is packaged with PyInstaller. Here\u0026rsquo;s is a brief description of PyInstaller, straight from its manual:\nPyInstaller bundles a Python application and all its dependencies into a single package. The user can run the packaged app without installing a Python interpreter or any modules.\nThere are also details about its inner workings:\n[\u0026hellip;] The bootloader is the heart of the one-file bundle also. When started it creates a temporary folder in the appropriate temp-folder location for this OS. The folder is named _MEI*xxxxxx*, where xxxxxx is a random number. [\u0026hellip;] The bootloader uncompresses the support files and writes copies into the temporary folder.\nAt 23:28:52, The folder _MEI86322 was created in my user\u0026rsquo;s Temp directory, which indicates I executed a PyInstaller executable. INDXRipper was kind enough to parse the $I30 of this folder for us, so we can see a compiled python module (.pyd file) that was created inside the folder and deleted since.\nMy guess is that floss64.exe was first executed at 23:14:57, then executed again at 23:25:35, and then again at 23:28:52. The $I30 entry we found in slack space was unallocated somewhere between 23:25:35 and 23:28:52, so the last modification time wasn\u0026rsquo;t updated the next time floss64.exe was executed.\nAn interesting thing to note, is that we only see a single _MEI*xxxxxx* folder; yet, we know a unique one should have been created for every execution of floss64.exe.\nWhen the bundled code terminates, the bootloader deletes the temporary folder. [\u0026hellip;] The _MEI*xxxxxx* folder is not removed if the program crashes or is killed\nThis is interesting! The folder _MEI86322 is not deleted, so floss64.exe might have crashed after it was executed at 23:28:52. I looked for an error report, but couldn\u0026rsquo;t find one.\nPrefetch Hash Cracking Can we find the folder from which floss64.exe was executed? Unfortunately, ShimCache and AmCache aren\u0026rsquo;t much help this time. A cool thing we can try is to brute-force the Prefetch hash; i.e, hashing possible full paths of floss64.exe until the result matches the Prefetch hash, which is F5F88991. I wrote a little tool to do this for me; I just have to give it the bodyfile I used to create the timeline, so it can enumerate all the folders floss64.exe might have been executed from:\nThis doesn\u0026rsquo;t always work, but this time it did! floss64.exe was executed from the Forensics directory on the C: drive.\nWrapping Up That\u0026rsquo;s it for this post! I wanted to demonstrate both the value and the subtleties of $I30 analysis; let me know if you found that educational. Also, I\u0026rsquo;m no expert; If you think I got something wrong, please tell me. Home Adventures may become a series if I find the time for it. I may also write a detailed post about INDXRipper; Let me know if you\u0026rsquo;d be interested in that!\nTool List Tool Source MFTECmd https://github.com/EricZimmerman/MFTECmd INDXRipper https://github.com/harelsegev/INDXRipper Prefetch Hash Cracker https://github.com/harelsegev/prefetch-hash-cracker ","permalink":"https://harelsegev.github.io/posts/home-adventures/home-adventures-a-prefetch-file-in-i30-slack-pyinstaller-prefetch-hash-cracking/","summary":"I often test my tools on my old computer at home. It\u0026rsquo;s so much more interesting to investigate than a newly created virtual machine. Today, while testing, I found evidence of activity from almost 2 years ago. It got me really excited, and I thought it would make a cool blog post!\nHere\u0026rsquo;s a snippet from the timeline I created using MFTECmd and INDXRipper:\nA Prefetch File in $I30 Slack The Prefetch file FLOSS64.","title":"Home Adventures! A Prefetch File in $I30 Slack, PyInstaller \u0026 Prefetch Hash Cracking"}]